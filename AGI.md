For me, the best single [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence) test for LLMs is open-ended novel problem solving in unfamiliar domains.

Present the model with a completely new problem type it hasn't encountered during training, requiring it to combine multiple reasoning modes whilst adapting to constraints it's never seen before.

The test works like this: Create a scenario that blends logical reasoning, creative problem-solving, and practical constraints from an obscure field or invented domain. The model must understand the rules, form hypotheses, test them mentally, and reach a solution without any direct training examples to draw from.

For example: "You're designing a communication system for underwater colonies using bioluminescent organisms. The organisms can only produce three colours, have a 2-second delay between signals, and die after 10 signals. Design an efficient language system for 50 common concepts whilst accounting for predator interference patterns."

It reveals true intelligence because:

+ It tests pure reasoning ability without memorised patterns.
+ It exposes whether the model can genuinely understand constraints and trade-offs rather than pattern-matching.
+ It reveals if the system can create novel solutions that demonstrate actual comprehension versus sophisticated mimicry.

The key advantage over other tests (like mathematics or coding) is that those domains have extensive training data. This test demands genuine problem-solving architecture rather than sophisticated retrieval and recombination.

A truly intelligent system should excel at this regardless of the specific domain chosen because it tests the fundamental capacity to reason through novel situations, which is the hallmark of general intelligence.